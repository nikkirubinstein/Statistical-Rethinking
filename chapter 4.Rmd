---
title: "Statistical Rethinking - Chapter 4"
author: "Nikki Rubinstein"
output:
  html_document: 
    toc: yes
  html_notebook: 
    theme: united
    toc: yes
---

# Linear Models

Linear regression attempts to learn about the mean and variance of a measurement using a Gaussian distribution to describe uncertainty.

## 4.1 Why normal distributions are normal

### 4.1.1 Normal by addition

Simulating 1000 experiments of 16 coin flips, with a step of randomly chosen size between 0 and 1 taken to the left for heads and right for tails. The resultant distribution is normal

```{r}
pos <- replicate(1000, sum(runif(16, -1, 1)))
hist(pos)
plot(density(pos))
```

Any process that adds together random values from the same distribution converges to normal - the central limit theorem.

### 4.1.2 Normal by multiplication

Simulating 10,000 experiments in which the effects of 12 alleles multiply to generate a growth factor. Convergence to a normal distribution occurs because the effect of multiplying small numbers approximates addition.

```{r}
growth <- replicate(10000, prod(1 + runif(12, 0, 0.1)))
plot(density(growth))
```
 

```{r}
big <- replicate (10000, prod(1 + runif(12, 0, 0.5)))
small <- replicate(10000, prod(1 + runif(12, 0, 0.01)))
plot(density(big), main = "Big values")
plot(density(small), main = "Small values")
```

### 4.1.3 Normal by log-multiplication

Although multiplying large deviates doesn't produce a Gaussian distribution on a linear scale, it does produce a Gaussian distribution when converted to a log scale. This is because adding logs is the equivalent of multiplying the original numbers.

```{r}
log.big <- replicate(10000, log(prod(1 + runif(12, 0, 0.5))))
plot(density(log.big), main = "Big values on a log scale")
```


### 4.1.4 Using Gaussian distributions

The Gaussian is a member of a family of fundamental natural distributions known as the exponential family (ontological justification). The Gaussian distribution is the shape that can be realised in the largest number of ways and does not introduce any new assumptions (epistemological justification). The Gaussian distribution is essentially the exponent of a negative quadratic.

## 4.2 A language for describing models

Requirements for a model:
1. Outcome variables
2. Gaussian likelihood distribution (the plausability of individual observations of the outcome variables)
3. Predictor variables
4. Define model parameters (the relationship between the likelihood function and the predictor variables)
5. Priors for all model paramters

### 4.2.1 Re-describing the globe tossing model

The posterior probability of p is proportional to the product of the likelihood (binomial distribution with 6 out of a possible 9) and the prior (uniform distribution between 0 and 1).

```{r}
w <- 6
n <- 9
p_grid <- seq(from = 0, to = 1, length.out = 100)
posterior <- dbinom(w, n, p_grid) * dunif(p_grid, 0, 1)
posterior <- posterior / sum(posterior)
plot(posterior)
```

## 4.3 A Gaussian model of height

### 4.3.1 The data

Partial census data for the Dobe area !Kung San in the 1960s: height - cms; weight - kgs; age - years; and maleness - 1 male, 0 female.
```{r}
library(rethinking)
library(dplyr)
data(Howell1)
d <- Howell1
# filter out people under 18 years of age
d2 <- d %>% filter(age >= 18)
```

### 4.3.2 The model

Adult heights are nearly always approximately normal.

```{r}
# The distribution of heights
hist(d2$height)
```

h<sub>i</sub> ~ Normal($\mu , \sigma$)

This model is often described as being independent and identically distributed (IID). This is an epistemological assumption, inherent to the model, not necessarily the external world.

We also need priors for the $\mu$ and $\sigma$ parameters of the above likelihood functoin.

$\mu$ ~ Normal(178, 20)

$\sigma$ ~ Uniform(0, 50)

```{r}
plot(100:250, dnorm(100:250, 178, 20), main = "Prior for mu", type = "l")

plot(-10:60, dunif(-10:60, 0, 50), type = "l", main = "Prior for sigma")
```

Heights can be simulated by sampling from the prior distribution.

```{r}
sample_mu <- rnorm(10000, 178, 20)
sample_sigma <- runif(10000, 0, 50)
prior_h <- rnorm(10000, sample_mu, sample_sigma)
hist(prior_h)
```

### 4.3.3 Grid approximation of the posterior distribution

The posterior distribution of a Gaussian by brute force...

```{r}
mu.list <- seq(from = 140, to = 160, length.out = 200)
sigma.list <- seq(from = 4, to = 9, length.out = 200)
post <- expand.grid(mu = mu.list, sigma = sigma.list)
post$LL <- sapply(1:nrow(post), function (i) {
  sum(dnorm(
    d2$height,
    mean = post$mu[i],
    sd = post$sigma[i],
    log = TRUE
  ))
})
post$prod <- post$LL + dnorm(post$mu, 178, 20, TRUE) + 
  dunif(post$sigma, 0, 50, TRUE)
post$prob <- exp(post$prod - max(post$prod))
# plot result
rethinking::contour_xyz(post$mu, post$sigma, post$prob)
rethinking::image_xyz(post$mu, post$sigma, post$prob)
```

### 4.3.4 Sampling from the posterior

```{r}
sample.rows <- sample(1:nrow(post), size = 10000, replace = TRUE, prob = post$prob)
sample.mu <- post$mu[sample.rows]
sample.sigma <- post$sigma[sample.rows]
# Plot samples
plot(sample.mu, sample.sigma, cex=0.5, pch = 16, col = col.alpha(rangi2, 0.1), xlab = "samples of mu", ylab = "samples of sigma")
```

Summarising the samples from the posterior distribution. As sample size increases, posterior density approaches the normal distribution.
```{r}
hist(sample.mu)
hist(sample.sigma)
```

The highest posterior density intervals occur at:
```{r}
rethinking::HPDI(sample.mu)
rethinking::HPDI(sample.sigma)
```

If you care about $\sigma$ you need to be careful of abusing the quadratic approximation, as the posterior deistribution of the standard deviation tends to have a long right tail, especially with smaller sample sizes.

```{r}
d3 <- sample(d2$height, size = 20)

# unsure why the values of from and to for the mu.list and sigma.list are changed from the previous example
mu.list <- seq(from = 150, to = 170, length.out = 200)
sigma.list <- seq(from = 4, to = 20, length.out = 200)
post2 <- expand.grid(mu = mu.list, sigma = sigma.list)
post2$LL <- sapply(1:nrow(post2), function (i) {
  sum(dnorm(
    d3,
    mean = post2$mu[i],
    sd = post2$sigma[i],
    log = TRUE
  ))
})
post2$prod <- post2$LL + dnorm(post2$mu, 178, 20, TRUE) + 
  dunif(post2$sigma, 0, 50, TRUE)
post2$prob <- exp(post2$prod - max(post2$prod))

sample2.rows <- sample(1:nrow(post2), size = 10000, replace = TRUE, prob = post2$prob)
sample2.mu <- post2$mu[sample2.rows]
sample2.sigma <- post2$sigma[sample2.rows]
# Plot samples
plot(sample2.mu, sample2.sigma, cex=0.5, pch = 16, col = col.alpha(rangi2, 0.1), xlab = "samples of mu", ylab = "samples of sigma")
dens(sample2.sigma, norm.comp = TRUE)

```

### 4.3.5 Fitting the model with `map`

The quadratic approximation allows us to make quick inferences about the shape of the posterior, with the peak of the posterior lying at the maximum a posteriori and the shape of the posterior distribution is provided by the quadratic approximation at this peak.

```{r}
d <- Howell1
d2 <- d %>% filter(age >= 18)

flist <- alist(
  height ~ dnorm(mu, sigma),
  mu ~ dnorm(178, 20),
  sigma ~ dunif(0, 50)
)

# the maximum a posteriori model
m4.1 <- rethinking::map(flist = flist, data = d2)
rethinking::precis(m4.1, prob = 0.95)
```

The above numbers provide Gaussian approximations for each parameter's marginal distribution.

The `map` function from the `rethinking` library uses a gradient ascent method to find the maximum a posteriori. The starting point is chosen randomly unless otherwise specified with a list of starting parameter values.

Note: list() and alist() both create list objects, but list() evaluates the code within it, while alist() does not.

Using a very narrow (informative) prior distribution restricts the posterior distribution.

```{r}
m4.2 <- rethinking::map(
  alist(
    height ~ dnorm(mu, sigma),
    mu ~ dnorm(170, 0.1),
    sigma ~ dunif(0, 50)
  ),
  data = d2
)
precis(m4.2)
```

The implied amount of data represented by a Gaussian prior for a given value of $\mu$ can be calculated can be calculated from the posterior $\sigma$:

$n = 1 / \sigma^2$

So a strong prior ($\mu$ ~ Normal(178, 0.1)) is equivalent to having 100 height values of 178cm, while a weaker prior ($\mu$ ~ Normal(178, 20)) implies 0.0025 of an observation.

### 4.3.6 Sampling from a `map` fit

A quadratic approximation of a posterior distribution with >1 parameter is a multi-dimensional Gaussian distribution. A list of means, standard deviations and a matrix of covariances ares sufficient to describe a multi-dimensional Gaussian distribution.

```{r}
# The variance-covariance matrix for model m4.1
vcov(m4.1)
```

The variance-covariance matrix tells us how each paramter in the posterior distribution relates to every other parameter. 

```{r}
# The vector of variances for the parameters
diag (vcov(m4.1))

# The correlatinon matrix
cov2cor(vcov(m4.1))
```

The correlation matrix shows that $\mu$ and $\sigma$ are not correlated with each other.

Sampling from a multidimensional Gaussian distribution involves sampling vecotrs of values. This preserves any covariance between $\mu$ and $\sigma$.

```{r}
library(MASS)
post <- MASS::mvrnorm(n = 10000, mu = coef(m4.1), Sigma = vcov(m4.1))
head(post)
plot(post)
```

Due to the skewed nature of the distribution of $\sigma$, it is common to estimate log($\sigma$) instead.

```{r}
# The exponent of log_sigma is used to ensure a positive value. The continuous log_sigma variable can be given a Gaussian prior
m4.1_logsigma <- map(
  alist(
    height ~ dnorm(mu, exp(log_sigma)),
    mu ~ dnorm(178, 20),
    log_sigma ~ dnorm(2, 10)
  ), data = d2
)

post <- MASS::mvrnorm(n = 10000, mu = coef(m4.1_logsigma), Sigma = vcov(m4.1_logsigma))
sigma <- exp(post[, 'log_sigma'])
```

## 4.4 Adding a predictor

Height vs weight

```{r}
plot(d2$height ~ d2$weight)
```

The term regression means using one or more predictor variables to model the distribution of one or more outcome variables.

### 4.4.1 The linear model strategy

Linear models assume that the predictor variables has a contstant and additive relationship with the mean of the outcome.

$h_i$ ~ Normal($\mu_i, \sigma$) [likelihood] \
$\mu_i = \alpha + \beta x_i$ [linear model] \
$\alpha$ ~ Normal(178, 100) [$\alpha$ prior] \
$\beta$ ~ Normal(0, 10)     [$\beta$ prior] \
$\sigma$ ~ Uniform(0, 50)   [$\sigma$ prior] 

For the likelihood calculation, the mean depends upon the predictor values in row $i$.

$\mu_i$ now has a direct deterministic relationship with $\alpha$, $\beta$ and the predictor variable $x_i$. The value $x_i$ is the weight of the invidividual on row $i$. $\alpha$ and $\beta$ are constants used to systematically vary $\mu$ across the distribution of the the data. $\alpha$ - also known as the intercept - describes the expected height when the weight is 0. $\beta$ describes the expected change in height when weight changes by a single unit. The model therefore finds a line relating weight to height.

$\beta = 0$ implies that there is no relationship between weight and height. Using a Gaussian prior centred at 0 is a more conservative approach than using a uniformly distributed prior. As the standard deviation of the prior decreases, the conservative influence increases.

### 4.4.2 Fitting the model

$h_i$ ~ Normal($\mu_i, \sigma$) ... `height ~ dnorm(mu, sigma)` \
$\mu_i = \alpha + \beta x_i$ ... `mu <- a + b*weight` \
$\alpha$ ~ Normal(178, 100) ... `a ~ dnorm(156, 100)` \
$\beta$ ~ Normal(0, 10) ... `b ~ dnorm(0, 10)` \
$\sigma$ ~ Uniform(0, 50) ... `sigma ~ dunif(0, 50)`

```{r}
m4.3 <- rethinking::map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b*weight,
    a ~ dnorm(156, 100),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ),
  data = d2
)
```

### 4.4.3 Interpreting the model fit

Plotting the implication of model estimates shows:
1. whether or not he model fitting procedure worked
2. the absolute magnitue of the relationship between outcome and predictor
3. the uncertainty surrounding the relationship
4. the uncertainty surrounding the implied predictions of the model

```{r}
# inspecting the model estimates gives the quadratic approximations of a, b and sigma
rethinking::precis(m4.3)
```

$\beta$ estimates the magnitude of the relationship between height and weight, i.e. a person 1kg heavier than another person is expected to be 90cm taller. 89% of the posterior probability lies between 0.84 and 0.97. So no relationship is very unlikely. As is a relationship greater than 1.

$\alpha$ estimates that a person of weight 0kg should be 114cm tall. Since a person of 0kg can't exist, this value needs to be interpreted within the context of the estimated value of $\beta$. $\sigma$ estimates the width of the distribution of heights around the mean. In this case, 95% of plausible heights lie within 9.5 - 10.8cm ($2\sigma$) of the mean height.

```{r}
# the correlation matrix of the model
rethinking::precis(m4.3, corr = TRUE)
cov2cor(vcov(m4.3))
```

Strong negative correlations, such as between $\alpha$ and $\beta$ above, can make it difficult to fit the model to the data in omre complex models.

**Centering** is the procedure of subtracting the mean of a variable from each value.

```{r}
d2 <- d2 %>%
  mutate(weight.c = weight - mean(weight))

m4.4 <- rethinking::map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * weight.c,
    a ~ dnorm(178, 100), 
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ),
  data = d2
)
rethinking::precis(m4.4, corr = TRUE)
mean(d2$height)
```

The estimates for $\beta$ and $\sigma$ are unchanged. But the value of $\alpha$ is now the same value as the mean height in the original data and correlations between parameters is now 0. Now $\alpha$ is the mean value of the outcome variable when the predictor is at its average value.

```{r}
# superimpose MAP values for mean height over actual data
plot(height ~ weight, data = d2)
abline(a = coef(m4.3)['a'], b = coef(m4.3)['b'])
```

The MAP line shown above gives the posterior mean; the most plausible line of all lines considered for the posterior distribution.

```{r}
# sampling from the model to create correlated random samples from the joint posterior of all 3 model parameters 
post <- rethinking::extract.samples(m4.3)
post[1:5,]

# display raw data
plot(d2$weight, d2$height,
     col = rangi2, xlab = "weight", ylab = "height", 
     main = paste('N = ', nrow(d2)))

# plot lines
for (i in 1:nrow(post))
  abline(a = post$a[i], b = post$b[i], col = col.alpha("black", 0.01))
```

The uncertainty is shown by sampling lines from the posterior distribution. Greater uncertainty is seen for extreme values of weight.

It is more common to display uncertainty by plotting an interval or contour around the MAP regression line.

```{r}
# samples from the posterior for a person weighing 50kg
mu_at_50 <- post$a + post$b * 50
head(mu_at_50)
hist(mu_at_50, xlab = "mu|weight = 50", main = "")
rethinking::HPDI(mu_at_50)
```

Since adding two Gaussians yields a Gaussian, the posterior distribution of mu (addition of a and b) is Gaussian.

```{r}
# compute mu from the model posterior distribution for each data (height) observation
weight.seq = 25:70
mu <- link(m4.3, data = data.frame(weight = weight.seq))
str(mu)

# alternate method not using rethinking library
mu_alt <- sapply(weight.seq, function(i) post$a + post$b * i)
str(mu_alt)

plot(height ~ weight, d2, type = "n")
for (i in 1:nrow(mu))
  points(weight.seq, mu[i,], pch = 16, col = col.alpha(rangi2,0.1))

# alternate method not using rethinking library
plot(height ~ weight, d2, type = "n")
for (i in 1:nrow(mu_alt))
  points(weight.seq, mu_alt[i,], pch = 16, col = col.alpha(rangi2,0.1))

```

```{r}
# summarise the distribution of mu
mu.mean <- sapply(data.frame(mu), mean)
mu.HPDI <- sapply(data.frame(mu), HPDI, prob = 0.89)

# plot raw data
plot(height ~ weight, data = d2, col = col.alpha(rangi2, 0.5))

# plot shaded region for 89% HPDI
rethinking::shade(mu.HPDI, weight.seq)

# plot MAP line
lines(weight.seq, mu.mean)
```

How to generate predictions and intervals from the posterior of a model fit:
1. generate distributions of posterior values of $\mu$ across all possible data values (i.e. the range of the x-axis)
2. Use summary functions like `mean`, `HPDI` or `PI` to find averages and lower and upper bounds of $\mu$ for each value of the predictor variable
3. Plot MAP line and surrounding intervals

Be careful when interpreting models and confidence intervals... *conditional on the assumption that height and weight are related by a straight line, then this is the most plausible line and these are is plausible bounds.*

```{r}
# simulated heights for each possible weight value, using sigma sampled from the posterior distribution
# sim.height <- rethinking::sim(m4.3, data = list(weight = weight.seq), n = 10000)
sim.height <- sapply(weight.seq, function(w)
  rnorm(
    n = nrow(post),
    mean = post$a + post$b * w,
    sd = post$sigma
  ))
str(sim.height)

# the 89% posterior prediction of interval of observable heights across weights 25:70kg
height.PI <- sapply(data.frame(sim.height), PI)

# plotting the results
plot(height ~ weight, d2, col = col.alpha(rangi2, 0.5))
lines(weight.seq, mu.mean)
# HPDI region for line
rethinking::shade(mu.HPDI, weight.seq)
# PI region for simulated heights
rethinking::shade(height.PI, weight.seq)
```

## 4.5 Polynomial regression

```{r}
library(ggplot2)
ggplot(d, aes(x = weight, y = height)) + geom_point()
```

Polynomial regressions are generally a bad idea because they are hard to interpret. The most common polynomial regression is a parabolic (second order) model of the mean:

$\mu_i = \alpha + \beta_1x_i + \beta_2x_i^2$

$\beta_2$ measures the curvature of the relationship between the predictor and outcome variables. 

The first part to fitting a parabolic model is standardising the predictor variable: subtracting the mean and dividing by the standard deviation.

```{r}
# standardise weights
d <- d %>% 
  mutate(weight.s = (weight - mean(weight)) / sd(weight),
         weight.s2 = weight.s^2,
         weight.s3 = weight.s^3)
```

The parabolic model (with weak priors):

$h_i$ ~ Normal($\mu_i, \sigma$) ... `height ~ dnorm(mu, sigma)` \
$\mu_i = \alpha + \beta_i x_i + \beta_2x_i^2$ ... `mu <- a + b1*weight.s + b2*weight.s^2` \
$\alpha$ ~ Normal(178, 100) ... `a ~ dnorm(156, 100)` \
$\beta_1$ ~ Normal(0, 10) ... `b1 ~ dnorm(0, 10)` \
$\beta_2$ ~ Normal(0, 10) ... `b2 ~ dnorm(0, 10)` \
$\sigma$ ~ Uniform(0, 50) ... `sigma ~ dunif(0, 50)`

```{r}
# fitting the parabolic model
m4.5 <- rethinking::map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b1*weight.s + b2*weight.s2,
    a ~ dnorm(156, 100),
    b1 ~ dnorm(0, 10),
    b2 ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ),
  data = d
)

# summary of estimates
rethinking::precis(m4.5)

# generating results for plotting
weight.seq <- seq(from = -2.2, to = 2, length.out = 30)
pred_dat <- list(weight.s = weight.seq, weight.s2 = weight.seq^2)
mu <- link(m4.5, data = pred_dat)
mu.mean <- sapply(data.frame(mu), mean)
mu.PI <- sapply(data.frame(mu), PI)
sim.height <- sim(m4.5, data = pred_dat)
height.PI <- sapply(data.frame(sim.height), PI)

# plotting results
plot(height ~ weight.s, d, col = col.alpha(rangi2, 0.5))
lines(weight.seq, mu.mean)
shade(mu.PI, weight.seq)
shade(height.PI, weight.seq)
```

```{r}
# Fitting a cubic regression model
m4.6 <- rethinking::map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b1*weight.s + b2*weight.s2 + b3*weight.s3,
    a ~ dnorm(156, 100),
    b1 ~ dnorm(0, 10),
    b2 ~ dnorm(0, 10),
    b3 ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ),
  data = d
)

# generating results for plotting
weight.seq <- seq(from = -2.2, to = 2, length.out = 30)
pred_dat <- list(weight.s = weight.seq, weight.s2 = weight.seq^2, weight.s3 = weight.seq^3)
mu <- link(m4.6, data = pred_dat)
mu.mean <- sapply(data.frame(mu), mean)
mu.PI <- sapply(data.frame(mu), PI)
sim.height <- sim(m4.6, data = pred_dat)
height.PI <- sapply(data.frame(sim.height), PI)

# plotting results
plot(height ~ weight.s, d, col = col.alpha(rangi2, 0.5))
lines(weight.seq, mu.mean)
shade(mu.PI, weight.seq)
shade(height.PI, weight.seq)
```

# Questions

## Easy

### 4E1 

Question: In the model definition below, which line is the likelihood?
$y_i$ ~ Normal($\mu, \sigma$) \
$\mu$ ~ Normal(0, 10) \ 
$\sigma$ ~ Uniform(0, 10)

Answer: $y_i$ ~ Normal($\mu, \sigma$) 

### 4E2

Question: In the model definition above, how many parameters are in the posterior distribution?

Answer: Two ($\mu$ and $\sigma$)

### 4E3

Question: Using the model definition above, write down the appropriate form of the Bayes' theroem that includes the proper likelihood and priors.

Answer: 