---
title: "Statistical Rethinking - Chapter 4"
author: "Nikki Rubinstein"
output:
  html_document: 
    toc: yes
  html_notebook: 
    theme: united
    toc: yes
---

# Linear Models

Linear regression attempts to learn about the mean and variance of a measurement using a Gaussian distribution to describe uncertainty.

## 4.1 Why normal distributions are normal

### 4.1.1 Normal by addition

Simulating 1000 experiments of 16 coin flips, with a step of randomly chosen size between 0 and 1 taken to the left for heads and right for tails. The resultant distribution is normal

```{r}
pos <- replicate(1000, sum(runif(16, -1, 1)))
hist(pos)
plot(density(pos))
```

Any process that adds together random values from the same distribution converges to normal - the central limit theorem.

### 4.1.2 Normal by multiplication

Simulating 10,000 experiments in which the effects of 12 alleles multiply to generate a growth factor. Convergence to a normal distribution occurs because the effect of multiplying small numbers approximates addition.

```{r}
growth <- replicate(10000, prod(1 + runif(12, 0, 0.1)))
plot(density(growth))
```
 

```{r}
big <- replicate (10000, prod(1 + runif(12, 0, 0.5)))
small <- replicate(10000, prod(1 + runif(12, 0, 0.01)))
plot(density(big), main = "Big values")
plot(density(small), main = "Small values")
```

### 4.1.3 Normal by log-multiplication

Although multiplying large deviates doesn't produce a Gaussian distribution on a linear scale, it does produce a Gaussian distribution when converted to a log scale. This is because adding logs is the equivalent of multiplying the original numbers.

```{r}
log.big <- replicate(10000, log(prod(1 + runif(12, 0, 0.5))))
plot(density(log.big), main = "Big values on a log scale")
```


### 4.1.4 Using Gaussian distributions

The Gaussian is a member of a family of fundamental natural distributions known as the exponential family (ontological justification). The Gaussian distribution is the shape that can be realised in the largest number of ways and does not introduce any new assumptions (epistemological justification). The Gaussian distribution is essentially the exponent of a negative quadratic.

## 4.2 A language for describing models

Requirements for a model:
1. Outcome variables
2. Gaussian likelihood distribution (the plausability of individual observations of the outcome variables)
3. Predictor variables
4. Define model parameters (the relationship between the likelihood function and the predictor variables)
5. Priors for all model paramters

### 4.2.1 Re-describing the globe tossing model

The posterior probability of p is proportional to the product of the likelihood (binomial distribution with 6 out of a possible 9) and the prior (uniform distribution between 0 and 1).

```{r}
w <- 6
n <- 9
p_grid <- seq(from = 0, to = 1, length.out = 100)
posterior <- dbinom(w, n, p_grid) * dunif(p_grid, 0, 1)
posterior <- posterior / sum(posterior)
plot(posterior)
```

## 4.3 A Gaussian model of height

### 4.3.1 The data

Partial census data for the Dobe area !Kung San in the 1960s: height - cms; weight - kgs; age - years; and maleness - 1 male, 0 female.
```{r}
library(rethinking)
library(dplyr)
data(Howell1)
d <- Howell1
# filter out people under 18 years of age
d2 <- d %>% filter(age >= 18)
```

### 4.3.2 The model

Adult heights are nearly always approximately normal.

```{r}
# The distribution of heights
hist(d2$height)
```

h<sub>i</sub> ~ Normal($\mu , \sigma$)

This model is often described as being independent and identically distributed (IID). This is an epistemological assumption, inherent to the model, not necessarily the external world.

We also need priors for the $\mu$ and $\sigma$ parameters of the above likelihood functoin.

$\mu$ ~ Normal(178, 20)

$\sigma$ ~ Uniform(0, 50)

```{r}
plot(100:250, dnorm(100:250, 178, 20), main = "Prior for mu", type = "l")

plot(-10:60, dunif(-10:60, 0, 50), type = "l", main = "Prior for sigma")
```

Heights can be simulated by sampling from the prior distribution.

```{r}
sample_mu <- rnorm(10000, 178, 20)
sample_sigma <- runif(10000, 0, 50)
prior_h <- rnorm(10000, sample_mu, sample_sigma)
hist(prior_h)
```

### 4.3.3 Grid approximation of the posterior distribution

The posterior distribution of a Gaussian by brute force...

```{r}
mu.list <- seq(from = 140, to = 160, length.out = 200)
sigma.list <- seq(from = 4, to = 9, length.out = 200)
post <- expand.grid(mu = mu.list, sigma = sigma.list)
post$LL <- sapply(1:nrow(post), function (i) {
  sum(dnorm(
    d2$height,
    mean = post$mu[i],
    sd = post$sigma[i],
    log = TRUE
  ))
})
post$prod <- post$LL + dnorm(post$mu, 178, 20, TRUE) + 
  dunif(post$sigma, 0, 50, TRUE)
post$prob <- exp(post$prod - max(post$prod))
# plot result
rethinking::contour_xyz(post$mu, post$sigma, post$prob)
rethinking::image_xyz(post$mu, post$sigma, post$prob)
```

### 4.3.4 Sampling from the posterior

```{r}
sample.rows <- sample(1:nrow(post), size = 10000, replace = TRUE, prob = post$prob)
sample.mu <- post$mu[sample.rows]
sample.sigma <- post$sigma[sample.rows]
# Plot samples
plot(sample.mu, sample.sigma, cex=0.5, pch = 16, col = col.alpha(rangi2, 0.1), xlab = "samples of mu", ylab = "samples of sigma")
```

Summarising the samples from the posterior distribution. As sample size increases, posterior density approaches the normal distribution.
```{r}
hist(sample.mu)
hist(sample.sigma)
```

The highest posterior density intervals occur at:
```{r}
rethinking::HPDI(sample.mu)
rethinking::HPDI(sample.sigma)
```

If you care about $\sigma$ you need to be careful of abusing the quadratic approximation, as the posterior deistribution of the standard deviation tends to have a long right tail, especially with smaller sample sizes.

```{r}
d3 <- sample(d2$height, size = 20)

# unsure why the values of from and to for the mu.list and sigma.list are changed from the previous example
mu.list <- seq(from = 150, to = 170, length.out = 200)
sigma.list <- seq(from = 4, to = 20, length.out = 200)
post2 <- expand.grid(mu = mu.list, sigma = sigma.list)
post2$LL <- sapply(1:nrow(post2), function (i) {
  sum(dnorm(
    d3,
    mean = post2$mu[i],
    sd = post2$sigma[i],
    log = TRUE
  ))
})
post2$prod <- post2$LL + dnorm(post2$mu, 178, 20, TRUE) + 
  dunif(post2$sigma, 0, 50, TRUE)
post2$prob <- exp(post2$prod - max(post2$prod))

sample2.rows <- sample(1:nrow(post2), size = 10000, replace = TRUE, prob = post2$prob)
sample2.mu <- post2$mu[sample2.rows]
sample2.sigma <- post2$sigma[sample2.rows]
# Plot samples
plot(sample2.mu, sample2.sigma, cex=0.5, pch = 16, col = col.alpha(rangi2, 0.1), xlab = "samples of mu", ylab = "samples of sigma")
dens(sample2.sigma, norm.comp = TRUE)

```

### 4.3.5 Fitting the model with `map`

The quadratic approximation allows us to make quick inferences about the shape of the posterior, with the peak of the posterior lying at the maximum a posteriori and the shape of the posterior distribution is provided by the quadratic approximation at this peak.

```{r}
d <- Howell1
d2 <- d %>% filter(age >= 18)

flist <- alist(
  height ~ dnorm(mu, sigma),
  mu ~ dnorm(178, 20),
  sigma ~ dunif(0, 50)
)

# the maximum a posteriori model
m4.1 <- rethinking::map(flist = flist, data = d2)
rethinking::precis(m4.1, prob = 0.95)
```

The above numbers provide Gaussian approximations for each parameter's marginal distribution.

The `map` function from the `rethinking` library uses a gradient ascent method to find the maximum a posteriori. The starting point is chosen randomly unless otherwise specified with a list of starting parameter values.

Note: list() and alist() both create list objects, but list() evaluates the code within it, while alist() does not.

Using a very narrow (informative) prior distribution restricts the posterior distribution.

```{r}
m4.2 <- rethinking::map(
  alist(
    height ~ dnorm(mu, sigma),
    mu ~ dnorm(170, 0.1),
    sigma ~ dunif(0, 50)
  ),
  data = d2
)
precis(m4.2)
```

